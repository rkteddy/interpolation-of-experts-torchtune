# Qwen2.5-0.5B-Instruct + MoE MLP (Top-2 placeholder) on alpaca_cleaned.
#
# This config is intended for fast iteration and ROCm compatibility.
#
# Download the base model with torchtune (example):
#   tune download Qwen/Qwen2.5-0.5B-Instruct --output-dir /data/checkpoints/Qwen2.5-0.5B-Instruct
# Then set checkpoint_dir below accordingly.

# -----------------------------
# Paths
# -----------------------------
checkpoint_dir: /data/checkpoints/Qwen2.5-0.5B-Instruct
output_dir: /data/outputs/qwen2_5_0_5b_moe_alpaca_cleaned

# -----------------------------
# Runtime
# -----------------------------
device: cuda          # ROCm uses the 'cuda' device type in PyTorch
dtype: bf16

# -----------------------------
# Model (MoE)
# -----------------------------
model:
  _component_: ioe_torchtune.models.qwen2_5_moe.qwen2_5_0_5b_moe
  num_experts: 8
  top_k: 2
  capacity_factor: null
  router_z_loss_coef: 0.0
  expert_dropout: 0.0

# This weight multiplies aux_loss inside the recipe:
load_balancing_loss_weight: 0.01

# The recipe will auto-detect whether labels need shifting.
# Options: auto | true | false
shift_labels: auto

ignore_index: -100

# -----------------------------
# Tokenizer (Qwen2.5)
# -----------------------------
tokenizer:
  _component_: torchtune.models.qwen2_5.qwen2_5_tokenizer
  path: ${checkpoint_dir}/vocab.json
  merges_file: ${checkpoint_dir}/merges.txt
  max_seq_len: 512

# -----------------------------
# Data (alpaca_cleaned)
# -----------------------------
dataset:
  _component_: torchtune.datasets.alpaca_cleaned_dataset
  train_on_input: false
  max_seq_len: 512
  split: train
  shuffle: true

batch_size: 1
num_workers: 2
epochs: 1
gradient_accumulation_steps: 8
clip_grad_norm: 1.0

# -----------------------------
# Optimizer
# -----------------------------
optimizer:
  _component_: torch.optim.AdamW
  lr: 2.0e-5
  betas: [0.9, 0.95]
  eps: 1.0e-8
  weight_decay: 0.1
  fused: false   # fused=True is often CUDA-only; keep false for ROCm

# -----------------------------
# Checkpoint loading (HF)
# -----------------------------
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: ${checkpoint_dir}
  checkpoint_files: [model.safetensors]
  output_dir: ${output_dir}
  model_type: QWEN2

# -----------------------------
# FSDP (FSDP2 via torchtune.training.shard_model)
# -----------------------------
enable_fsdp: true
fsdp_cpu_offload: false
fsdp_reshard_after_forward: true

# -----------------------------
# Logging
# -----------------------------
metric_logger:
  # TensorBoard is self-contained and works well on clusters.
  _component_: torchtune.training.metric_logging.TensorBoardLogger
  log_dir: ${output_dir}/tb
  log_every_n_steps: 5

  # --- Alternative: Weights & Biases ---
  # _component_: torchtune.training.metric_logging.WandBLogger
  # project: "qwen2_5-moe"
  # name: "0.5b-top2gating"
  # save_dir: ${output_dir}
  # log_every_n_steps: 5
